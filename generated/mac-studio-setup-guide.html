<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Orchid Mac Studio Setup Guide â€” Autonomous Server Runbook</title>
<style>
  :root {
    --bg: #0d1117;
    --surface: #161b22;
    --border: #30363d;
    --text: #e6edf3;
    --text-muted: #8b949e;
    --accent: #58a6ff;
    --accent-green: #3fb950;
    --accent-orange: #d29922;
    --accent-red: #f85149;
    --accent-purple: #bc8cff;
    --code-bg: #0d1117;
  }
  * { margin: 0; padding: 0; box-sizing: border-box; }
  body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif; background: var(--bg); color: var(--text); line-height: 1.7; padding: 2rem; max-width: 960px; margin: 0 auto; }
  h1 { font-size: 2rem; margin-bottom: 0.5rem; border-bottom: 1px solid var(--border); padding-bottom: 1rem; }
  h2 { font-size: 1.5rem; margin-top: 2.5rem; margin-bottom: 1rem; color: var(--accent); border-bottom: 1px solid var(--border); padding-bottom: 0.5rem; }
  h3 { font-size: 1.15rem; margin-top: 1.5rem; margin-bottom: 0.5rem; color: var(--accent-purple); }
  p, li { margin-bottom: 0.5rem; }
  ul, ol { padding-left: 1.5rem; margin-bottom: 1rem; }
  code { background: var(--code-bg); border: 1px solid var(--border); border-radius: 4px; padding: 0.15em 0.4em; font-size: 0.9em; font-family: 'SF Mono', SFMono-Regular, Consolas, 'Liberation Mono', Menlo, monospace; }
  pre { background: var(--code-bg); border: 1px solid var(--border); border-radius: 8px; padding: 1rem; overflow-x: auto; margin: 1rem 0; font-size: 0.85rem; line-height: 1.5; }
  pre code { border: none; padding: 0; background: none; }
  .badge { display: inline-block; padding: 0.15em 0.6em; border-radius: 12px; font-size: 0.75rem; font-weight: 600; text-transform: uppercase; letter-spacing: 0.05em; }
  .badge-green { background: rgba(63,185,80,0.15); color: var(--accent-green); border: 1px solid rgba(63,185,80,0.3); }
  .badge-orange { background: rgba(210,153,34,0.15); color: var(--accent-orange); border: 1px solid rgba(210,153,34,0.3); }
  .badge-red { background: rgba(248,81,73,0.15); color: var(--accent-red); border: 1px solid rgba(248,81,73,0.3); }
  .badge-blue { background: rgba(88,166,255,0.15); color: var(--accent); border: 1px solid rgba(88,166,255,0.3); }
  .phase { background: var(--surface); border: 1px solid var(--border); border-radius: 8px; padding: 1.5rem; margin: 1.5rem 0; }
  .phase-header { display: flex; align-items: center; gap: 0.75rem; margin-bottom: 1rem; }
  .phase-num { background: var(--accent); color: var(--bg); width: 28px; height: 28px; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: 700; font-size: 0.85rem; flex-shrink: 0; }
  .callout { border-left: 3px solid; padding: 0.75rem 1rem; margin: 1rem 0; border-radius: 0 6px 6px 0; }
  .callout-warn { border-color: var(--accent-orange); background: rgba(210,153,34,0.08); }
  .callout-danger { border-color: var(--accent-red); background: rgba(248,81,73,0.08); }
  .callout-info { border-color: var(--accent); background: rgba(88,166,255,0.08); }
  .callout-success { border-color: var(--accent-green); background: rgba(63,185,80,0.08); }
  table { width: 100%; border-collapse: collapse; margin: 1rem 0; font-size: 0.9rem; }
  th, td { border: 1px solid var(--border); padding: 0.5rem 0.75rem; text-align: left; }
  th { background: var(--surface); font-weight: 600; }
  .meta { color: var(--text-muted); font-size: 0.85rem; margin-bottom: 2rem; }
  a { color: var(--accent); text-decoration: none; }
  a:hover { text-decoration: underline; }
  .toc { background: var(--surface); border: 1px solid var(--border); border-radius: 8px; padding: 1.25rem 1.5rem; margin: 1.5rem 0; }
  .toc ul { list-style: none; padding-left: 0; margin-bottom: 0; }
  .toc li { padding: 0.2rem 0; }
  .toc a { color: var(--text-muted); }
  .toc a:hover { color: var(--accent); }
  hr { border: none; border-top: 1px solid var(--border); margin: 2rem 0; }
  .source-tag { font-size: 0.75rem; color: var(--text-muted); font-style: italic; }
  .checklist li { list-style: none; padding-left: 0; }
  .checklist li::before { content: "â˜ "; }
</style>
</head>
<body>

<h1>ğŸ–¥ï¸ Orchid Mac Studio â€” Autonomous Server Setup Guide</h1>
<p class="meta">
  Synthesized from ~2 weeks of team discussions (Jan 19 â€“ Feb 17, 2026)<br>
  Contributors: Matt Elder (tooling &amp; architecture), DK (security &amp; sysadmin), Nolan (hardware research), David S. (requirements)<br>
  Generated by ORI Â· Feb 17, 2026
</p>

<div class="toc">
  <strong>Table of Contents</strong>
  <ul>
    <li><a href="#hardware">0. Hardware Spec &amp; Purchase Decision</a></li>
    <li><a href="#phase1">1. Initial macOS Setup &amp; Hardening</a></li>
    <li><a href="#phase2">2. Network Security &amp; Remote Access</a></li>
    <li><a href="#phase3">3. Developer Environment &amp; Toolchains</a></li>
    <li><a href="#phase4">4. GitHub Actions Self-Hosted Runner</a></li>
    <li><a href="#phase5">5. sccache â€” Shared Build Cache</a></li>
    <li><a href="#phase6">6. Ollama â€” Local LLM Inference</a></li>
    <li><a href="#phase7">7. RustDesk OSS â€” Remote Desktop</a></li>
    <li><a href="#phase8">8. ORI Bot Hosting</a></li>
    <li><a href="#phase9">9. QA Automation Environment</a></li>
    <li><a href="#phase10">10. Autonomous Operations &amp; Monitoring</a></li>
    <li><a href="#phase11">11. User Accounts &amp; Access Control</a></li>
    <li><a href="#checklist">12. Day-1 Launch Checklist</a></li>
  </ul>
</div>

<!-- ============================================================ -->
<h2 id="hardware">0. Hardware Spec &amp; Purchase Decision</h2>

<table>
  <tr><th>Attribute</th><th>Value</th></tr>
  <tr><td>Model</td><td>Mac Studio M4 Max (2025)</td></tr>
  <tr><td>CPU</td><td>16-core (12P + 4E)</td></tr>
  <tr><td>GPU</td><td>40-core Apple GPU</td></tr>
  <tr><td>Unified Memory</td><td>128 GB</td></tr>
  <tr><td>Storage</td><td>1 TB SSD (consider 2 TB for $200 bump)</td></tr>
  <tr><td>Memory Bandwidth</td><td>546 GB/s</td></tr>
  <tr><td>Ports</td><td>4Ã— Thunderbolt 5, 2Ã— USB-A, 2Ã— USB-C, HDMI, 10Gb Ethernet, SDXC, 3.5mm</td></tr>
  <tr><td>Price</td><td>~$3,699</td></tr>
</table>

<div class="callout callout-info">
  <strong>Why this config?</strong> Team consensus (Matt, David, Nolan, DK â€” Feb 8-11 discussions): M4 Max 128GB is the sweet spot. Newest generation, maxed single-die, great power profile. 128GB runs 70B models via Ollama at ~20 tok/s. Skip the M3 Ultra (overkill + old gen) and wait for M5 Ultra mid-year if you need 2-die architecture later.
</div>

<h3>Intended Workloads (Prioritized)</h3>
<table>
  <tr><th>#</th><th>Workload</th><th>RAM Budget</th><th>Priority</th></tr>
  <tr><td>1</td><td>Self-hosted CI Runner (GitHub Actions)</td><td>16-24 GB</td><td><span class="badge badge-red">P0</span></td></tr>
  <tr><td>2</td><td>sccache shared build cache</td><td>4-8 GB</td><td><span class="badge badge-red">P0</span></td></tr>
  <tr><td>3</td><td>Ollama local LLM inference</td><td>20-40 GB</td><td><span class="badge badge-orange">P1</span></td></tr>
  <tr><td>4</td><td>ORI bot hosting</td><td>4-8 GB</td><td><span class="badge badge-orange">P1</span></td></tr>
  <tr><td>5</td><td>QA automation (CDP + headless Chrome)</td><td>8-16 GB</td><td><span class="badge badge-orange">P1</span></td></tr>
  <tr><td>6</td><td>Remote dev target (SSH)</td><td>Shared</td><td><span class="badge badge-blue">P2</span></td></tr>
  <tr><td>7</td><td>RustDesk relay (self-hosted)</td><td>&lt;1 GB</td><td><span class="badge badge-blue">P2</span></td></tr>
  <tr><td>8</td><td>Edge AI experimentation</td><td>Variable</td><td><span class="badge badge-green">P3</span></td></tr>
</table>

<!-- ============================================================ -->
<h2 id="phase1">1. Initial macOS Setup &amp; Hardening</h2>

<div class="phase">
<div class="phase-header"><div class="phase-num">1</div><strong>First Boot &amp; System Configuration</strong></div>

<h3>1.1 Create Admin Account</h3>
<pre><code># During Setup Assistant:
# Account name: orchid-admin
# DO NOT use a personal Apple ID â€” create a dedicated org Apple ID or skip
# Enable FileVault encryption when prompted</code></pre>

<h3>1.2 System Settings</h3>
<pre><code># Prevent sleep (this machine runs 24/7)
sudo pmset -a sleep 0
sudo pmset -a disksleep 0
sudo pmset -a displaysleep 10    # display can sleep
sudo pmset -a hibernatemode 0
sudo pmset -a autopoweroff 0

# Restart automatically after power failure
sudo pmset -a autorestart 1

# Verify
pmset -g</code></pre>

<h3>1.3 macOS Hardening</h3>
<pre><code># Enable the firewall
sudo /usr/libexec/ApplicationFirewall/socketfilterfw --setglobalstate on
sudo /usr/libexec/ApplicationFirewall/socketfilterfw --setloggingmode on
sudo /usr/libexec/ApplicationFirewall/socketfilterfw --setstealthmode on

# Disable unnecessary services
sudo launchctl disable system/com.apple.AirPlayXPCHelper
sudo launchctl disable system/com.apple.screensharing

# Enable automatic security updates
sudo defaults write /Library/Preferences/com.apple.SoftwareUpdate AutomaticallyInstallMacOSUpdates -bool true
sudo defaults write /Library/Preferences/com.apple.SoftwareUpdate CriticalUpdateInstall -bool true</code></pre>

<h3>1.4 File Descriptor Limits (Critical for Qdrant)</h3>
<div class="callout callout-warn">
  <strong>Known issue:</strong> macOS GUI apps inherit launchd's 256 FD limit. Qdrant with 4 collections + HTTP/gRPC easily exceeds this. This has caused silent crashes (see <a href="https://github.com/OrchidStudio/orchid/issues/662">ORC #662</a>).
</div>
<pre><code># Permanent fix (survives reboot)
sudo tee /Library/LaunchDaemons/limit.maxfiles.plist &lt;&lt;'EOF'
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd"&gt;
&lt;plist version="1.0"&gt;
&lt;dict&gt;
  &lt;key&gt;Label&lt;/key&gt;
  &lt;string&gt;limit.maxfiles&lt;/string&gt;
  &lt;key&gt;ProgramArguments&lt;/key&gt;
  &lt;array&gt;
    &lt;string&gt;launchctl&lt;/string&gt;
    &lt;string&gt;limit&lt;/string&gt;
    &lt;string&gt;maxfiles&lt;/string&gt;
    &lt;string&gt;65536&lt;/string&gt;
    &lt;string&gt;200000&lt;/string&gt;
  &lt;/array&gt;
  &lt;key&gt;RunAtLoad&lt;/key&gt;
  &lt;true/&gt;
&lt;/dict&gt;
&lt;/plist&gt;
EOF

sudo launchctl load -w /Library/LaunchDaemons/limit.maxfiles.plist

# Verify
launchctl limit maxfiles
# Expected: maxfiles 65536 200000</code></pre>

<h3>1.5 Machine Naming</h3>
<pre><code>sudo scutil --set ComputerName "orchid-studio"
sudo scutil --set HostName "orchid-studio"
sudo scutil --set LocalHostName "orchid-studio"</code></pre>
</div>

<!-- ============================================================ -->
<h2 id="phase2">2. Network Security &amp; Remote Access</h2>

<div class="phase">
<div class="phase-header"><div class="phase-num">2</div><strong>Zero-Trust Network Architecture</strong></div>

<div class="callout callout-danger">
  <strong>DK's Security Advisory (Feb 17):</strong> "As a former SysAdmin, I'd want to know if an end user was setting up a machine for remote access by outside devs to ensure it's segmented properly from other internal systems... Network segmentation if you are giving remote access to the machine with full control of the OS. Having its own Virtual LAN or having robust firewall access rules should work well."
</div>

<h3>2.1 Tailscale (Zero-Trust VPN Mesh)</h3>
<p>David has already used Tailscale + Termius for remote access. This is the primary network layer â€” <strong>no ports are exposed to WAN</strong>.</p>
<pre><code># Install Tailscale
brew install --cask tailscale

# Start and authenticate
# Use the Orchid team Tailscale account
# Enable MagicDNS so the machine is reachable as "orchid-studio"

# Verify
tailscale status
tailscale ip -4    # Get the Tailscale IP (100.x.x.x)</code></pre>

<h3>2.2 SSH Hardening (Tailscale-Only)</h3>
<pre><code># Enable Remote Login in System Settings â†’ General â†’ Sharing â†’ Remote Login
# Then restrict SSH to Tailscale interface only:

sudo tee -a /etc/ssh/sshd_config.d/orchid.conf &lt;&lt;'EOF'
# Only listen on Tailscale interface
ListenAddress 100.0.0.0/8

# Key-based auth only â€” NO passwords
PasswordAuthentication no
ChallengeResponseAuthentication no
PubkeyAuthentication yes

# Restrict to orchid team users
AllowUsers orchid-admin matt david dk nolan

# Hardening
PermitRootLogin no
MaxAuthTries 3
LoginGraceTime 30
ClientAliveInterval 300
ClientAliveCountMax 2

# Use modern crypto only
KexAlgorithms curve25519-sha256,curve25519-sha256@libssh.org
Ciphers chacha20-poly1305@openssh.com,aes256-gcm@openssh.com
MACs hmac-sha2-256-etm@openssh.com,hmac-sha2-512-etm@openssh.com
EOF

# Restart SSH
sudo launchctl kickstart -k system/com.openssh.sshd</code></pre>

<h3>2.3 SSH Key Distribution</h3>
<pre><code># For each team member, add their public key:
mkdir -p ~/.ssh
chmod 700 ~/.ssh

# Add keys (one per team member)
cat &gt;&gt; ~/.ssh/authorized_keys &lt;&lt;'EOF'
# David Sadofsky (david)
ssh-ed25519 AAAA... david@orchid

# Matthew Elder (matt)
ssh-ed25519 AAAA... matt@orchid

# David Karnowski (dk)
ssh-ed25519 AAAA... dk@orchid

# Nolan Makatche (nolan)
ssh-ed25519 AAAA... nolan@orchid
EOF

chmod 600 ~/.ssh/authorized_keys</code></pre>

<div class="callout callout-info">
  <strong>DK's recommendation:</strong> Consider <a href="https://www.obdev.at/products/littlesnitch/">Little Snitch</a> ($49) for granular per-process firewall control. "I like to use it when testing new apps or other features as a check on what is actually calling out to remote servers. The visual network monitor is kinda rad too."
</div>

<h3>2.4 Network Segmentation (DK's Requirement)</h3>
<pre><code># Option A: Tailscale ACLs (recommended â€” software-defined)
# In your Tailscale admin console, create ACL rules:
# - orchid-studio can only accept connections from team members
# - orchid-studio outbound: GitHub, npm registry, crates.io, Ollama models
# - Block access to other devices on the LAN

# Option B: Router-level VLAN (if hardware supports it)
# Put orchid-studio on its own VLAN (e.g., VLAN 10)
# Firewall rules: allow Tailscale UDP 41641, deny all other inbound
# Allow outbound: HTTPS (443), DNS (53), NTP (123)

# Option C: macOS pf firewall (defense in depth)
sudo tee /etc/pf.anchors/orchid &lt;&lt;'EOF'
# Block all inbound except Tailscale and loopback
block in all
pass in on lo0 all
pass in on utun+ all          # Tailscale interface
pass out all keep state        # Allow all outbound
EOF

# Load the anchor
echo 'anchor "orchid"' | sudo tee -a /etc/pf.conf
echo 'load anchor "orchid" from "/etc/pf.anchors/orchid"' | sudo tee -a /etc/pf.conf
sudo pfctl -f /etc/pf.conf -e</code></pre>
</div>

<!-- ============================================================ -->
<h2 id="phase3">3. Developer Environment &amp; Toolchains</h2>

<div class="phase">
<div class="phase-header"><div class="phase-num">3</div><strong>Install Core Tools</strong></div>

<h3>3.1 Xcode &amp; Command Line Tools</h3>
<pre><code># Install Xcode CLT (required for Rust, Tauri builds)
xcode-select --install

# If full Xcode is needed for cidre/CoreAudio builds:
# Install from App Store, then:
sudo xcode-select -s /Applications/Xcode.app/Contents/Developer
sudo xcodebuild -license accept</code></pre>

<h3>3.2 Homebrew</h3>
<pre><code>/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
echo 'eval "$(/opt/homebrew/bin/brew shellenv)"' &gt;&gt; ~/.zprofile
eval "$(/opt/homebrew/bin/brew shellenv)"</code></pre>

<h3>3.3 Rust Toolchain</h3>
<pre><code># Install rustup
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
source "$HOME/.cargo/env"

# Install stable (matches Orchid repo: 1.93.0)
rustup default 1.93.0
rustup component add rustfmt clippy

# Install nightly for linting (pinned to match CI)
rustup install nightly-2026-02-05
rustup component add rustfmt --toolchain nightly-2026-02-05

# Install targets
rustup target add aarch64-apple-darwin
rustup target add x86_64-apple-darwin

# Verify
rustc --version    # 1.93.0
cargo --version</code></pre>

<h3>3.4 Node.js &amp; Frontend Dependencies</h3>
<pre><code># Install Node 20 LTS via brew
brew install node@20
echo 'export PATH="/opt/homebrew/opt/node@20/bin:$PATH"' &gt;&gt; ~/.zprofile

# Verify
node --version   # v20.x
npm --version</code></pre>

<h3>3.5 Build Dependencies</h3>
<pre><code># For Tauri / WebRTC / audio builds
brew install pkg-config cairo pango gdk-pixbuf webp giflib cmake ninja

# For sccache
brew install sccache

# Git & GitHub CLI
brew install git gh

# For CI runner
brew install jq</code></pre>

<h3>3.6 Clone Orchid Repository</h3>
<pre><code># Authenticate with GitHub
gh auth login

# Clone
mkdir -p ~/Projects
cd ~/Projects
gh repo clone OrchidStudio/orchid
cd orchid

# Verify build
cd frontend
npm install
npm run check    # All lint/type/format checks</code></pre>
</div>

<!-- ============================================================ -->
<h2 id="phase4">4. GitHub Actions Self-Hosted Runner</h2>

<div class="phase">
<div class="phase-header"><div class="phase-num">4</div><strong>CI Runner Setup</strong></div>

<div class="callout callout-success">
  <strong>Cost savings:</strong> GitHub charges 3-10x premium for macOS runners vs Linux. Self-hosted = $0 per minute. At current velocity (76 issues/cycle, 8-12 builds/day), this saves ~$3,000-9,000/year.
</div>

<pre><code># Create runner directory (NOT in ~/Documents â€” macOS permission issues)
mkdir -p ~/actions-runner
cd ~/actions-runner

# Download latest runner (check https://github.com/actions/runner/releases)
curl -o actions-runner-osx-arm64-2.329.0.tar.gz -L \
  https://github.com/actions/runner/releases/download/v2.329.0/actions-runner-osx-arm64-2.329.0.tar.gz

# Validate checksum
echo "50c0d409040cc52e701ac1d5afb4672cb7803a65c1292a30e96c42051dfa690f  actions-runner-osx-arm64-2.329.0.tar.gz" | shasum -a 256 -c

# Extract
tar xzf ./actions-runner-osx-arm64-2.329.0.tar.gz

# Get registration token from:
# https://github.com/OrchidStudio/orchid/settings/actions/runners/new
# Tokens expire in 1 hour!

# Configure
./config.sh \
  --url https://github.com/OrchidStudio/orchid \
  --token YOUR_TOKEN_HERE \
  --name orchid-studio \
  --labels self-hosted,macOS,ARM64,studio \
  --unattended

# Install as launchd service (auto-start on boot)
./svc.sh install

# Start
./svc.sh start

# Verify
./svc.sh status
gh api repos/OrchidStudio/orchid/actions/runners --jq '.runners[] | {name, status}'</code></pre>

<h3>Workflow Configuration</h3>
<pre><code># In .github/workflows/*.yml:
jobs:
  build:
    runs-on: [self-hosted, macOS, ARM64, studio]
    # With automatic fallback:
    # runs-on: ${{ github.repository_owner == 'OrchidStudio' && 'self-hosted' || 'macos-latest' }}</code></pre>

<div class="callout callout-warn">
  <strong>Concurrent PR handling:</strong> With one runner, PRs queue. Set <code>timeout-minutes: 30</code> to avoid stale jobs. If queuing becomes a bottleneck, add a second runner or reserve the Studio for macOS-specific builds only and use GitHub-hosted for platform-agnostic work.
</div>
</div>

<!-- ============================================================ -->
<h2 id="phase5">5. sccache â€” Shared Build Cache</h2>

<div class="phase">
<div class="phase-header"><div class="phase-num">5</div><strong>50-70% Faster Clean Builds</strong></div>

<pre><code># Install sccache (already done in Phase 3)
brew install sccache

# Option A: Local disk cache (simplest â€” start here)
export SCCACHE_DIR="$HOME/.sccache"
export SCCACHE_CACHE_SIZE="30G"
export RUSTC_WRAPPER="sccache"

# Option B: GCS bucket (shared across all dev machines)
export SCCACHE_GCS_BUCKET="orchid-sccache"
export SCCACHE_GCS_RW_MODE="READ_WRITE"
export SCCACHE_GCS_KEY_PATH="$HOME/.config/gcloud/orchid-sccache-key.json"
export RUSTC_WRAPPER="sccache"

# Add to shell profile
cat &gt;&gt; ~/.zprofile &lt;&lt;'EOF'
export SCCACHE_DIR="$HOME/.sccache"
export SCCACHE_CACHE_SIZE="30G"
export RUSTC_WRAPPER="sccache"
EOF

# Start the sccache daemon
sccache --start-server

# Verify
sccache --show-stats</code></pre>

<div class="callout callout-info">
  <strong>Expected impact:</strong> First Rust build populates the cache (~15 min). Subsequent clean builds drop to 4-6 minutes. With 128GB RAM, the OS page cache further accelerates repeated builds.
</div>
</div>

<!-- ============================================================ -->
<h2 id="phase6">6. Ollama â€” Local LLM Inference</h2>

<div class="phase">
<div class="phase-header"><div class="phase-num">6</div><strong>Local AI Model Serving</strong></div>

<p>Purpose: Offload high-frequency, low-complexity tasks from cloud APIs (vision, log analysis, code review, QA automation). Heavy tasks (Opus-class reasoning, Deepgram STT) stay on cloud.</p>

<pre><code># Install Ollama
brew install ollama

# Create launchd service for auto-start
sudo tee /Library/LaunchDaemons/com.ollama.server.plist &lt;&lt;'EOF'
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd"&gt;
&lt;plist version="1.0"&gt;
&lt;dict&gt;
  &lt;key&gt;Label&lt;/key&gt;
  &lt;string&gt;com.ollama.server&lt;/string&gt;
  &lt;key&gt;ProgramArguments&lt;/key&gt;
  &lt;array&gt;
    &lt;string&gt;/opt/homebrew/bin/ollama&lt;/string&gt;
    &lt;string&gt;serve&lt;/string&gt;
  &lt;/array&gt;
  &lt;key&gt;RunAtLoad&lt;/key&gt;
  &lt;true/&gt;
  &lt;key&gt;KeepAlive&lt;/key&gt;
  &lt;true/&gt;
  &lt;key&gt;EnvironmentVariables&lt;/key&gt;
  &lt;dict&gt;
    &lt;key&gt;OLLAMA_HOST&lt;/key&gt;
    &lt;string&gt;0.0.0.0:11434&lt;/string&gt;
    &lt;key&gt;OLLAMA_MAX_LOADED_MODELS&lt;/key&gt;
    &lt;string&gt;3&lt;/string&gt;
    &lt;key&gt;OLLAMA_NUM_PARALLEL&lt;/key&gt;
    &lt;string&gt;4&lt;/string&gt;
  &lt;/dict&gt;
  &lt;key&gt;StandardOutPath&lt;/key&gt;
  &lt;string&gt;/var/log/ollama.log&lt;/string&gt;
  &lt;key&gt;StandardErrorPath&lt;/key&gt;
  &lt;string&gt;/var/log/ollama.error.log&lt;/string&gt;
&lt;/dict&gt;
&lt;/plist&gt;
EOF

sudo launchctl load -w /Library/LaunchDaemons/com.ollama.server.plist

# Pull recommended models for Orchid workloads
ollama pull llama3.1:70b-instruct-q4_K_M    # ~40GB â€” main workhorse
ollama pull llama3.2-vision:11b              # ~7GB â€” screenshot analysis / QA
ollama pull qwen2.5-coder:14b               # ~8GB â€” code review
ollama pull nomic-embed-text                 # &lt;1GB â€” embeddings

# Verify
curl http://localhost:11434/api/tags | jq '.models[].name'</code></pre>

<div class="callout callout-warn">
  <strong>Security:</strong> Ollama binds to <code>0.0.0.0:11434</code> to allow access from other team machines. This is safe because the macOS firewall + Tailscale ensure only VPN-connected devices can reach it. Never expose port 11434 to WAN.
</div>

<h3>Memory Budget</h3>
<table>
  <tr><th>Model</th><th>Size</th><th>Use Case</th></tr>
  <tr><td>llama3.1:70b Q4</td><td>~40 GB</td><td>General reasoning, code review</td></tr>
  <tr><td>llama3.2-vision:11b</td><td>~7 GB</td><td>QA screenshot comparison</td></tr>
  <tr><td>qwen2.5-coder:14b</td><td>~8 GB</td><td>Quick code analysis</td></tr>
  <tr><td>nomic-embed-text</td><td>&lt;1 GB</td><td>Embeddings for search</td></tr>
  <tr><td><strong>Total at peak</strong></td><td><strong>~56 GB</strong></td><td>Leaves ~72 GB for OS + other workloads</td></tr>
</table>
</div>

<!-- ============================================================ -->
<h2 id="phase7">7. RustDesk OSS â€” Remote Desktop</h2>

<div class="phase">
<div class="phase-header"><div class="phase-num">7</div><strong>Self-Hosted Remote Desktop (Free)</strong></div>

<p class="source-tag">Source: Matt's research (Feb 11, #s-y-s-c-a) â€” "we don't gotta pay for rustdesk either if we don't care about the web/security/user-mgmt features which we probably don't"</p>

<pre><code># RustDesk OSS (AGPL-3.0) â€” hbbs (rendezvous) + hbbr (relay)
# No official macOS ARM64 prebuilt binaries, so build from source

# Clone and build
cd ~/Projects
git clone https://github.com/rustdesk/rustdesk-server.git
cd rustdesk-server
cargo build --release

# The binaries are at:
# target/release/hbbs  (ID/rendezvous server)
# target/release/hbbr  (relay server)

# Create data directory
mkdir -p ~/rustdesk-data

# Create launchd service for hbbs
sudo tee /Library/LaunchDaemons/com.rustdesk.hbbs.plist &lt;&lt;'EOF'
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd"&gt;
&lt;plist version="1.0"&gt;
&lt;dict&gt;
  &lt;key&gt;Label&lt;/key&gt;
  &lt;string&gt;com.rustdesk.hbbs&lt;/string&gt;
  &lt;key&gt;ProgramArguments&lt;/key&gt;
  &lt;array&gt;
    &lt;string&gt;/Users/orchid-admin/Projects/rustdesk-server/target/release/hbbs&lt;/string&gt;
  &lt;/array&gt;
  &lt;key&gt;WorkingDirectory&lt;/key&gt;
  &lt;string&gt;/Users/orchid-admin/rustdesk-data&lt;/string&gt;
  &lt;key&gt;RunAtLoad&lt;/key&gt;
  &lt;true/&gt;
  &lt;key&gt;KeepAlive&lt;/key&gt;
  &lt;true/&gt;
&lt;/dict&gt;
&lt;/plist&gt;
EOF

# Create launchd service for hbbr
sudo tee /Library/LaunchDaemons/com.rustdesk.hbbr.plist &lt;&lt;'EOF'
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd"&gt;
&lt;plist version="1.0"&gt;
&lt;dict&gt;
  &lt;key&gt;Label&lt;/key&gt;
  &lt;string&gt;com.rustdesk.hbbr&lt;/string&gt;
  &lt;key&gt;ProgramArguments&lt;/key&gt;
  &lt;array&gt;
    &lt;string&gt;/Users/orchid-admin/Projects/rustdesk-server/target/release/hbbr&lt;/string&gt;
  &lt;/array&gt;
  &lt;key&gt;WorkingDirectory&lt;/key&gt;
  &lt;string&gt;/Users/orchid-admin/rustdesk-data&lt;/string&gt;
  &lt;key&gt;RunAtLoad&lt;/key&gt;
  &lt;true/&gt;
  &lt;key&gt;KeepAlive&lt;/key&gt;
  &lt;true/&gt;
&lt;/dict&gt;
&lt;/plist&gt;
EOF

sudo launchctl load -w /Library/LaunchDaemons/com.rustdesk.hbbs.plist
sudo launchctl load -w /Library/LaunchDaemons/com.rustdesk.hbbr.plist</code></pre>

<h3>Client Configuration</h3>
<pre><code># On each team member's machine, install RustDesk client:
# https://rustdesk.com/
#
# In RustDesk client settings:
# ID Server: [Tailscale IP of orchid-studio]  e.g., 100.x.x.x
# Relay Server: [Same Tailscale IP]
# Key: [Contents of ~/rustdesk-data/id_ed25519.pub on the Studio]
#
# Ports (all over Tailscale only, never WAN):
# TCP 21114-21119
# UDP 21116</code></pre>
</div>

<!-- ============================================================ -->
<h2 id="phase8">8. ORI Bot Hosting</h2>

<div class="phase">
<div class="phase-header"><div class="phase-num">8</div><strong>Move ORI Off David's Personal MacBook</strong></div>

<p>ORI currently runs on David's personal machine. Moving to the Studio provides dedicated compute, 24/7 uptime, and access to local Ollama for offloading cheap inference.</p>

<pre><code># Clone ORI to the Studio
# (Specific ORI deployment steps depend on current architecture)
# Key requirements:
# - Node.js 20+ environment
# - Access to GitHub API (gh CLI authenticated)
# - Access to Slack API (bot tokens in env)
# - Access to local Ollama at http://localhost:11434
# - Process manager for auto-restart (launchd or pm2)

# Install PM2 for process management
npm install -g pm2

# Example PM2 ecosystem config:
cat &gt; ~/ori/ecosystem.config.js &lt;&lt;'EOF'
module.exports = {
  apps: [{
    name: 'ori',
    script: './start.sh',
    cwd: '/Users/orchid-admin/ori',
    env: {
      NODE_ENV: 'production',
      OLLAMA_BASE_URL: 'http://localhost:11434',
    },
    restart_delay: 5000,
    max_restarts: 10,
    autorestart: true,
  }]
};
EOF

# Start and save for auto-restart
pm2 start ecosystem.config.js
pm2 save
pm2 startup    # Generates launchd config</code></pre>

<div class="callout callout-info">
  <strong>Future: Local inference for ORI.</strong> Once hosted on the Studio, ORI can route simple tasks (vision analysis, log parsing, Slack responses) to local Ollama instead of burning cloud API credits. Complex tasks (Opus-class reasoning) stay on Anthropic API.
</div>
</div>

<!-- ============================================================ -->
<h2 id="phase9">9. QA Automation Environment</h2>

<div class="phase">
<div class="phase-header"><div class="phase-num">9</div><strong>Persistent QA Box</strong></div>

<pre><code># Install Chrome for CDP-based automation
brew install --cask google-chrome

# Install Playwright (for E2E testing)
npx playwright install chromium

# Keep a persistent Orchid dev instance running
# This provides a live app for verification at all times

# In a tmux session or via launchd:
cd ~/Projects/orchid/frontend
npm run dev &amp;         # Frontend dev server
npm run sidecars &amp;    # Qdrant + audio server

# The QA environment is now always warm:
# - CDP server on port 5173
# - Qdrant on port 6333/6334
# - Audio server on port 3000</code></pre>
</div>

<!-- ============================================================ -->
<h2 id="phase10">10. Autonomous Operations &amp; Monitoring</h2>

<div class="phase">
<div class="phase-header"><div class="phase-num">10</div><strong>Keep It Running 24/7</strong></div>

<h3>10.1 Service Health Checks</h3>
<pre><code># Create a health check script
cat &gt; ~/scripts/healthcheck.sh &lt;&lt;'SCRIPT'
#!/bin/bash
set -euo pipefail

SLACK_WEBHOOK="${ORCHID_SLACK_WEBHOOK:-}"
FAILURES=()

check_service() {
  local name="$1" cmd="$2"
  if ! eval "$cmd" &gt;/dev/null 2&gt;&amp;1; then
    FAILURES+=("$name")
  fi
}

# Check all services
check_service "Ollama"        "curl -sf http://localhost:11434/api/tags"
check_service "GH Runner"     "pgrep -f 'Runner.Listener'"
check_service "sccache"       "sccache --show-stats"
check_service "RustDesk hbbs" "pgrep hbbs"
check_service "RustDesk hbbr" "pgrep hbbr"
check_service "Tailscale"     "tailscale status"

if [ ${#FAILURES[@]} -gt 0 ]; then
  MSG="ğŸš¨ orchid-studio health check FAILED: ${FAILURES[*]}"
  echo "$MSG"
  if [ -n "$SLACK_WEBHOOK" ]; then
    curl -sf -X POST "$SLACK_WEBHOOK" \
      -H 'Content-Type: application/json' \
      -d "{\"text\": \"$MSG\"}"
  fi
  exit 1
fi

echo "âœ… All services healthy at $(date)"
SCRIPT
chmod +x ~/scripts/healthcheck.sh</code></pre>

<h3>10.2 Scheduled Health Checks via launchd</h3>
<pre><code>sudo tee ~/Library/LaunchAgents/com.orchid.healthcheck.plist &lt;&lt;'EOF'
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd"&gt;
&lt;plist version="1.0"&gt;
&lt;dict&gt;
  &lt;key&gt;Label&lt;/key&gt;
  &lt;string&gt;com.orchid.healthcheck&lt;/string&gt;
  &lt;key&gt;ProgramArguments&lt;/key&gt;
  &lt;array&gt;
    &lt;string&gt;/Users/orchid-admin/scripts/healthcheck.sh&lt;/string&gt;
  &lt;/array&gt;
  &lt;key&gt;StartInterval&lt;/key&gt;
  &lt;integer&gt;300&lt;/integer&gt;
  &lt;key&gt;StandardOutPath&lt;/key&gt;
  &lt;string&gt;/var/log/orchid-healthcheck.log&lt;/string&gt;
&lt;/dict&gt;
&lt;/plist&gt;
EOF

launchctl load ~/Library/LaunchAgents/com.orchid.healthcheck.plist</code></pre>

<h3>10.3 Disk Space Monitoring</h3>
<pre><code># CI artifacts + Docker images + model weights fill up fast
# Add to the health check script:
DISK_USAGE=$(df -h / | awk 'NR==2{print $5}' | tr -d '%')
if [ "$DISK_USAGE" -gt 85 ]; then
  FAILURES+=("Disk usage at ${DISK_USAGE}%")
fi

# Prune old CI artifacts weekly
cat &gt; ~/scripts/weekly-cleanup.sh &lt;&lt;'SCRIPT'
#!/bin/bash
# Clean GitHub runner work directories older than 7 days
find ~/actions-runner/_work -maxdepth 2 -type d -mtime +7 -exec rm -rf {} +

# Clean sccache stale entries
sccache --trim --max-size 30G

# Clean old Ollama model layers
ollama list   # Review periodically
SCRIPT
chmod +x ~/scripts/weekly-cleanup.sh</code></pre>

<h3>10.4 Auto-Restart After Power Failure</h3>
<pre><code># Already set in Phase 1:
sudo pmset -a autorestart 1

# All services use RunAtLoad/KeepAlive in launchd, so they auto-recover:
# âœ… Ollama
# âœ… GitHub Actions runner
# âœ… RustDesk hbbs/hbbr
# âœ… Health check cron
# âœ… Tailscale</code></pre>
</div>

<!-- ============================================================ -->
<h2 id="phase11">11. User Accounts &amp; Access Control</h2>

<div class="phase">
<div class="phase-header"><div class="phase-num">11</div><strong>Per-User Isolation</strong></div>

<h3>Recommended Account Structure</h3>
<table>
  <tr><th>Account</th><th>Type</th><th>Purpose</th></tr>
  <tr><td><code>orchid-admin</code></td><td>Admin</td><td>System management, service installation, sole admin account</td></tr>
  <tr><td><code>matt</code></td><td>Standard</td><td>SSH dev access, Rust builds, Ollama usage</td></tr>
  <tr><td><code>david</code></td><td>Standard</td><td>SSH dev access, ORI management</td></tr>
  <tr><td><code>dk</code></td><td>Standard</td><td>SSH dev access, security auditing</td></tr>
  <tr><td><code>nolan</code></td><td>Standard</td><td>SSH dev access, QA testing</td></tr>
  <tr><td><code>_runner</code></td><td>Service</td><td>GitHub Actions runner (headless, no login shell)</td></tr>
</table>

<pre><code># Create user accounts
for user in matt david dk nolan; do
  sudo sysadminctl -addUser "$user" -fullName "$user" -shell /bin/zsh -home "/Users/$user"
  sudo mkdir -p "/Users/$user/.ssh"
  sudo chmod 700 "/Users/$user/.ssh"
  # Add their SSH public key
  # sudo tee "/Users/$user/.ssh/authorized_keys" &lt; /path/to/${user}_key.pub
  sudo chown -R "$user" "/Users/$user/.ssh"
done

# Each user gets their own:
# - Home directory with independent shell config
# - SSH key-based authentication
# - Shared access to /opt/homebrew tools
# - Shared access to Ollama API on localhost:11434</code></pre>
</div>

<!-- ============================================================ -->
<h2 id="checklist">12. Day-1 Launch Checklist</h2>

<div class="phase">
<div class="phase-header"><div class="phase-num">âœ“</div><strong>Verify Everything Works</strong></div>

<h3>System</h3>
<ul class="checklist">
  <li>FileVault encryption enabled</li>
  <li>macOS firewall enabled + stealth mode</li>
  <li><code>pmset</code> sleep disabled, auto-restart enabled</li>
  <li>File descriptor limits raised to 65536</li>
  <li>Machine named <code>orchid-studio</code></li>
  <li>Automatic security updates enabled</li>
</ul>

<h3>Network &amp; Security</h3>
<ul class="checklist">
  <li>Tailscale installed, authenticated, MagicDNS enabled</li>
  <li>SSH restricted to Tailscale interface only</li>
  <li>SSH keys deployed for all 4 team members</li>
  <li>Password authentication disabled</li>
  <li>No ports exposed to WAN</li>
  <li>Network segmentation in place (VLAN or Tailscale ACLs)</li>
  <li>Little Snitch installed (optional â€” DK's recommendation)</li>
</ul>

<h3>Developer Tools</h3>
<ul class="checklist">
  <li>Xcode CLT installed</li>
  <li>Homebrew installed</li>
  <li>Rust 1.93.0 stable + nightly-2026-02-05</li>
  <li>Node.js 20 LTS</li>
  <li>Orchid repo cloned and <code>npm run check</code> passes</li>
</ul>

<h3>Services (All Auto-Start on Boot)</h3>
<ul class="checklist">
  <li>GitHub Actions runner registered, online, labeled <code>self-hosted,macOS,ARM64,studio</code></li>
  <li>sccache running, stats showing</li>
  <li>Ollama serving, models pulled (llama3.1:70b, vision:11b, qwen-coder, nomic-embed)</li>
  <li>RustDesk hbbs + hbbr running</li>
  <li>Health check script running every 5 minutes</li>
  <li>Weekly cleanup cron scheduled</li>
</ul>

<h3>Access Verification</h3>
<ul class="checklist">
  <li>Each team member can SSH in: <code>ssh orchid-studio</code> (via Tailscale)</li>
  <li>Each team member can hit Ollama: <code>curl http://orchid-studio:11434/api/tags</code></li>
  <li>RustDesk client connects for GUI access</li>
  <li>GitHub shows runner as "Idle" (green)</li>
</ul>
</div>

<hr>

<h2>Architecture Diagram</h2>
<pre><code>
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    orchid-studio (Mac Studio M4 Max 128GB)       â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ GH Actions   â”‚  â”‚   Ollama     â”‚  â”‚    QA Automation     â”‚  â”‚
â”‚  â”‚ Runner       â”‚  â”‚ :11434       â”‚  â”‚  CDP + Chrome        â”‚  â”‚
â”‚  â”‚ (launchd)    â”‚  â”‚              â”‚  â”‚  Orchid dev server   â”‚  â”‚
â”‚  â”‚              â”‚  â”‚ 70B/11B/14B  â”‚  â”‚  Qdrant sidecar      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   sccache    â”‚  â”‚  RustDesk    â”‚  â”‚       ORI Bot        â”‚  â”‚
â”‚  â”‚ (30GB cache) â”‚  â”‚ hbbs + hbbr  â”‚  â”‚   (PM2 managed)      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    Tailscale VPN Mesh                      â”‚  â”‚
â”‚  â”‚              (zero-trust, all traffic encrypted)           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚              â”‚              â”‚              â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚  David  â”‚   â”‚   Matt  â”‚   â”‚    DK   â”‚   â”‚  Nolan  â”‚
    â”‚  (SSH)  â”‚   â”‚  (SSH)  â”‚   â”‚  (SSH)  â”‚   â”‚  (SSH)  â”‚
    â”‚(RDesk)  â”‚   â”‚(RDesk)  â”‚   â”‚(RDesk)  â”‚   â”‚(RDesk)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â—„â”€â”€â”€â”€ All connections via Tailscale. No WAN exposure. â”€â”€â”€â”€â–º
</code></pre>

<hr>

<h2>Key Management (Matt's Architecture)</h2>
<p class="source-tag">Source: Matt's key management design (Jan 21, private channel)</p>
<table>
  <tr><th>Tier</th><th>Where</th><th>Rotation</th><th>Who Manages</th></tr>
  <tr><td>devcloud</td><td>Google Secrets Manager (CMEK)</td><td>Auto 90 days</td><td>Infra (blind to devs)</td></tr>
  <tr><td>prodcloud</td><td>Google Secrets Manager</td><td>Auto 90 days</td><td>Infra (blind to devs)</td></tr>
  <tr><td>github</td><td>GitHub Actions Secrets</td><td>Manual</td><td>Admin</td></tr>
  <tr><td>devlocal (Studio)</td><td>1Password / <code>.env</code></td><td>Manual</td><td>Team</td></tr>
</table>

<div class="callout callout-warn">
  <strong>On the Studio:</strong> Use 1Password CLI (<code>op</code>) to inject secrets at runtime. Never store API keys in plaintext files. The <code>.env</code> should reference <code>op://</code> URIs. As Matt noted: "local dev keys â€” we will need less and less in the future as our server-side arch blossoms."
</div>

<hr>
<p style="text-align: center; color: var(--text-muted); font-size: 0.85rem; margin-top: 2rem;">
  ğŸ¤– Generated by ORI (Orchid Repository Intelligence) Â· Feb 17, 2026<br>
  Synthesized from team discussions in: #all-orchid-ai, #s-y-s-c-a, #mad-science, #engineering, #random, #ori-tasks
</p>

</body>
</html>
